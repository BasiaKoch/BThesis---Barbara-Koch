# ðŸ§  BThesis---Barbara-Koch

Welcome! This repository contains the code and supporting materials for my **Bachelorâ€™s thesis**, which investigates how modern deep learning architectures perform on the task of segmenting brain tumors in MRI scans. The project compares two state-of-the-art models: **nnU-Net** (based on convolutional neural networks) and **Swin UNETR** (a transformer-based model), using the **BraTS2020** dataset.

> ðŸ§© **Note**: This repository builds upon publicly available implementations from:
> - [MIC-DKFZ/nnUNet](https://github.com/MIC-DKFZ/nnUNet) â€“ the official nnU-Net codebase
> - [Inc0mple/3D_Brain_Tumor_Seg_V2](https://github.com/Inc0mple/3D_Brain_Tumor_Seg_V2) â€“ Swin UNETR training pipeline and structure

These frameworks provided the foundation for model training, testing, and comparison, which were adapted for the purposes of this thesis.

---

## ðŸŽ¯ Project Aim

The goal of this thesis was to evaluate and compare the performance of CNN-based and transformer-based approaches in the segmentation of glioma subregions:

- **Whole Tumor (WT)**
- **Tumor Core (TC)**
- **Enhancing Tumor (ET)**

By training both models under the same resource constraints and without pretrained weights, the project explores their strengths, limitations, and suitability for different clinical objectives.

---

## ðŸ—‚ Repository Structure

```text

This repository contains the full codebase and training pipeline for the bachelorâ€™s thesis project comparing **nnU-Net** and **Swin UNETR** architectures on the **BraTS2020** dataset for 3D multimodal brain tumor segmentation.

```text
BThesis---Barbara-Koch/
â”‚
â”œâ”€â”€ nnUnet/                           # nnU-Net training and evaluation pipeline
â”‚   â”œâ”€â”€ automated_pretraining/       # Preprocessing outputs
â”‚   â”‚   â”œâ”€â”€ created_fingerprint.json         # Dataset fingerprint (spacing, size, intensity)
â”‚   â”‚   â””â”€â”€ nnUNetPlans.json                # Training plans and architecture configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/                     # Shell scripts to run training and prediction
â”‚   â”‚   â”œâ”€â”€ train.sh
â”‚   â”‚   â”œâ”€â”€ predict.sh
â”‚   â”‚   â””â”€â”€ evaluate
â”‚   â”‚
â”‚   â”œâ”€â”€ testing/                     # Inference results and adapted prediction script
â”‚   â”‚   â”œâ”€â”€ predict_from_raw_data.py       # Adapted inference script from nnUNetv2
â”‚   â”‚   â”œâ”€â”€ evaluation.json                # Summary evaluation results (Dice/IoU)
â”‚   â”‚   â””â”€â”€ progress.png                   # Training progress visualization
â”‚   â”‚
â”‚   â”œâ”€â”€ train/                       # Training logic and config
â”‚   â”‚   â”œâ”€â”€ run_training.py                # Launch script for nnU-Net training
â”‚   â”‚   â”œâ”€â”€ train.json                     # Training configuration
â”‚   â”‚   â””â”€â”€ trainer.py                     # Trainer class
â”‚   â”‚
â”‚   â”œâ”€â”€ logs/                        # Training logs
â”‚   â”‚   â””â”€â”€ training_log.txt                # Epoch-wise performance and loss log
â”‚   â”‚
â”‚   â”œâ”€â”€ statistical_analysis/       # ðŸ“Š Plots and scripts for analyzing nnU-Net output
â”‚   â”‚   â”œâ”€â”€ all_stats.py                    # Computes full per-class metrics
â”‚   â”‚   â”œâ”€â”€ dice-iou_bar.py                # Barplot of Dice and IoU by region
â”‚   â”‚   â”œâ”€â”€ class_mean_summary_stats.csv   # Summary stats (mean, std) for each class
â”‚   â”‚   â”œâ”€â”€ per_case_metrics.csv           # Case-wise evaluation metrics
â”‚   â”‚   â”œâ”€â”€ error_rate_by_class.png        # Visualization of classification error per class
â”‚   â”‚   â””â”€â”€ iou_distribution.png           # Distribution of IoU scores
â”‚
â”œâ”€â”€ SWIN_UNETR/                    # Swin UNETR fine-tuning and evaluation
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ train.sh                      # Training shell script
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ BratsDataset.py              # BraTS dataset loader
â”‚   â”‚   â”œâ”€â”€ Meter.py                     # Metric logger
â”‚   â”‚   â”œâ”€â”€ fold_data.csv                # Fold splits for cross-validation
â”‚   â”‚   â””â”€â”€ viz_eval_utils.py            # Visualization utility functions
â”‚   â”‚
â”‚   â”œâ”€â”€ logs/                            # Logs and output of evaluation
â”‚   â”‚   â”œâ”€â”€ swinUNETR_test_results.csv           # Per-case results
â”‚   â”‚   â”œâ”€â”€ SwinUNETR_summary_statistics.csv     # Summary statistics
â”‚   â”‚   â”œâ”€â”€ SwinUNETR_dice_vs_jaccard_scatter.pdf # Scatterplot Dice vs Jaccard
â”‚   â”‚   â”œâ”€â”€ SwinUNETR_score_distributions.pdf    # Boxplot distributions
â”‚   â”‚   â”œâ”€â”€ metrics_barplot.png                  # Barplot of performance
â”‚   â”‚   â”œâ”€â”€ swinUNETR_trainer_properties.txt     # Full model config summary
â”‚   â”‚   â””â”€â”€ train_log_20250414-004602.csv        # Epoch-wise training log
â”‚   â”‚
â”‚   â”œâ”€â”€ models.py                     # Swin UNETR architecture (MONAI-based)
â”‚   â”œâ”€â”€ train.py                      # Training script for Swin UNETR
â”‚   â”œâ”€â”€ generate_your_json.py        # Script to generate dataset split JSON
â”‚   â””â”€â”€ vizualize_test.py            # ðŸ”¹ Adapted from Inc0mpleâ€™s visualization notebook
â”‚
â”œâ”€â”€ testing_significance/           # ðŸ“Š Statistical comparison of both models
â”‚   â”œâ”€â”€ boxplot_dice_iou_synthetic.py     # Dice/IoU boxplots on test sets
â”‚   â””â”€â”€ wilcoxon_synthetic_analysis.py    # Wilcoxon test for paired model performance
â”‚
â””â”€â”€ README.md                       # This file â€“ full project overview and documentation
             
```
## ðŸ§  Pretrained Model Weights

The pretrained weights for both segmentation models used in this project are available below:

- ðŸŒ€ **Swin UNETR** (Transformer-based):
  [Download `your_best_model_20250414-004601.pth`](https://drive.google.com/file/d/1lEjkvGCFt4yLCkP-OvhpKjnd4zrHOVT-/view?usp=drive_link)

- ðŸ§© **nnU-Net** (CNN-based):
  [Download `nnunet_best_model.pth`](https://drive.google.com/file/d/1HCK1qAsZj2TgxeGd8Rg4gVG81Z8gZodV/view?usp=sharing)

After downloading, place the weights in a suitable directory (e.g., `./weights/`) and update your loading scripts accordingly:

```python
# Example
model.load_state_dict(torch.load("weights/your_best_model_20250414-004601.pth"))

